{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# 查看数据集--小莎士比亚训练集\n",
    "with open('input.txt', 'r') as  f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、测试分词工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24]) # 打印分词后对应字典中的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、将分词后的文本变成小批量输入到模型\n",
    "- 创建小批量输入\n",
    "- 创建对应的标签，也就是向后错一位的输入x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "buf = torch.tensor(tokens[:24 + 1])     # 先转成tensor格式\n",
    "x = buf[:-1].view(4, 6)                 # 小批量的输入，直接调整张量形状就可以做到分出批量了\n",
    "y = buf[1:].view(4, 6)                  # 小批量对应的输入对应的标签\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、测试分词并分成小批量的文本，输入模型后的反应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2 import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 准备小批量数据\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "with open('input.txt', 'r') as  f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "tokens = enc.encode(data)\n",
    "\n",
    "B, T = 4, 32            # 批量为4， 序列长度为32\n",
    "\n",
    "import torch\n",
    "buf = torch.tensor(tokens[:B*T + 1])     # 先转成tensor格式\n",
    "x = buf[:-1].view(B, T)                 # 小批量的输入，直接调整张量形状就可以做到分出批量了\n",
    "y = buf[1:].view(B, T) \n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 50257])\n",
      "tensor(11.0398, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\deeplearn\\Project\\2_Myself  project\\28_GPT-2\\GPT2.py:59: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "# 查看输出和损失是否正常\n",
    "model = GPT(GPTConfig)\n",
    "model.to(device)\n",
    "logits, loss  = model(x, y)\n",
    " \n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: NVIDIA GeForce RTX 3090\n",
      "FlashAttention available: True\n",
      "torch version: <module 'torch.version' from 'd:\\\\miniconda\\\\envs\\\\gpt2\\\\Lib\\\\site-packages\\\\torch\\\\version.py'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Device name:\", torch.cuda.get_device_properties('cuda').name)\n",
    "print(\"FlashAttention available:\", torch.backends.cuda.flash_sdp_enabled())\n",
    "print(f'torch version: {torch.version}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
