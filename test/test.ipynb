{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken             # 导入分词工具\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载处理好token碎片文件\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    ppt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # 获取储存tokens的碎片文件名\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)              # 返回在data_root文件夹下文件名的列表\n",
    "        shards = [s for s in shards if split in s]  # 根据split参数，选取是train数据集还是value数据集\n",
    "        shards = sorted(shards)                     # 排序文件\n",
    "        shards = [os.path.join(data_root, s) for s in shards]   # 将shards列表中的文件名加上文件夹的名字，组成路径\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "\n",
    "        print(f\"found {len(shards)} shards for split {split}\")\n",
    "        self.reset()                                # 为什么要调用这个函数\n",
    "\n",
    "    def reset(self):\n",
    "        # \n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = 0 \n",
    "        print(f\"1 shard tokens:{len(self.tokens)}  batch:{self.B * self.T}\")\n",
    "        print(f\"1 shard = {len(self.tokens) // (self.B * self.T)} batchs\")\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1] # 加载一个小小批量\n",
    "        x = (buf[:-1]).view(B, T)                   # 训练集\n",
    "        y = (buf[1:]).view(B, T)                    # 标签\n",
    "        print(f\"position:{self.current_position} shard:{self.current_shard}\")\n",
    "\n",
    "        self.current_position += B * T \n",
    "\n",
    "        if self.current_position + (B * T  + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards) #  此处设置的特别好，是一个轮回\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard]) # 加载新的tokens碎片\n",
    "            self.current_position = 0\n",
    "            print(f\"新的碎片shard： {self.current_shard}\")\n",
    "        # print(f\"position:{self.current_position} shard:{self.current_shard}\")\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 99 shards for split train\n",
      "1 shard tokens:100000000  min——batch:3072000\n",
      "1 shard = 32 batchs\n",
      "position:0 shard:0\n",
      "position:3072000 shard:0\n",
      "position:6144000 shard:0\n",
      "position:9216000 shard:0\n",
      "position:12288000 shard:0\n",
      "position:15360000 shard:0\n",
      "position:18432000 shard:0\n",
      "position:21504000 shard:0\n",
      "position:24576000 shard:0\n",
      "position:27648000 shard:0\n",
      "position:30720000 shard:0\n",
      "position:33792000 shard:0\n",
      "position:36864000 shard:0\n",
      "position:39936000 shard:0\n",
      "position:43008000 shard:0\n",
      "position:46080000 shard:0\n",
      "position:49152000 shard:0\n",
      "position:52224000 shard:0\n",
      "position:55296000 shard:0\n",
      "position:58368000 shard:0\n",
      "position:61440000 shard:0\n",
      "position:64512000 shard:0\n",
      "position:67584000 shard:0\n",
      "position:70656000 shard:0\n",
      "position:73728000 shard:0\n",
      "position:76800000 shard:0\n",
      "position:79872000 shard:0\n",
      "position:82944000 shard:0\n",
      "position:86016000 shard:0\n",
      "position:89088000 shard:0\n",
      "position:92160000 shard:0\n",
      "position:95232000 shard:0\n",
      "新的碎片shard： 1\n",
      "position:0 shard:1\n",
      "position:3072000 shard:1\n",
      "position:6144000 shard:1\n",
      "position:9216000 shard:1\n",
      "position:12288000 shard:1\n",
      "position:15360000 shard:1\n",
      "position:18432000 shard:1\n",
      "position:21504000 shard:1\n",
      "position:24576000 shard:1\n",
      "position:27648000 shard:1\n",
      "position:30720000 shard:1\n",
      "position:33792000 shard:1\n",
      "position:36864000 shard:1\n",
      "position:39936000 shard:1\n",
      "position:43008000 shard:1\n",
      "position:46080000 shard:1\n",
      "position:49152000 shard:1\n",
      "position:52224000 shard:1\n"
     ]
    }
   ],
   "source": [
    "train_data = DataLoaderLite(3000, 1024, 'train')\n",
    "for i in range(50):\n",
    "\n",
    "    x, y= train_data.next_batch()\n",
    "    # print(x, y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite_input():\n",
    "    \n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # 从训练数据中加载数据\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # 先进行分词\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "\n",
    "        # 将分词后的token转为tensor格式\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "        \n",
    "        # 打印显示有多少tokens\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batchs\")\n",
    "\n",
    "        # 标记已经用过的数据\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        # 准备输入和标签\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "\n",
    "        print(f\"position : {self.current_position}\")\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        # 下一批用新的数据\n",
    "        self.current_position += B * T\n",
    "\n",
    "        # 如果1个epoch用完了，则计数器归零\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "            print(\"新的轮回\")\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "1 epoch = 6 batchs\n",
      "position : 0\n",
      "position : 50000\n",
      "position : 100000\n",
      "position : 150000\n",
      "position : 200000\n",
      "position : 250000\n",
      "新的轮回\n",
      "position : 0\n",
      "position : 50000\n",
      "position : 100000\n",
      "position : 150000\n"
     ]
    }
   ],
   "source": [
    "train_data = DataLoaderLite_input(50, 1000)\n",
    "for i in range(10):\n",
    "    x, y= train_data.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288\n",
      ">= calculated gradient accumulation steps: 32\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524288 # 2**19,约0.5M的token数量\n",
    "B = 16 # 小batch的大小，3090只能支持16，教程中可以是64\n",
    "T = 1024\n",
    "assert total_batch_size % (B * T) == 0 # 确保大批量是小批量的整数倍！\n",
    "grad_accum_steps = total_batch_size // (B * T) # 这个是统计需要进行累积小批量的轮数\n",
    "print(f\"total desired batch size: {total_batch_size}\")\n",
    "print(f\">= calculated gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
